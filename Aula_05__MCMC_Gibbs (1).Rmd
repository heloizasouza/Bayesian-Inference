---
title: "Aula 5: Amostrador de Gibbs"
author: "Prof. Dr. Eder Angelo Milani"
date: "02/08/2023"
output: 
  pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# O amostrador de Gibbs

O amostrador de Gibbs é frequentemente aplicado quando a distribuição de interesse é uma distribuição multivariada. Suponha que todas as densidades condicionais univariadas são conhecidas e é razoavelmente fácil de amostra delas. Então a cadeia é gerada por amostragem das distribuições marginais da distribuição desejada, **e todo candidato é aceito**. 


**Algoritmo de Gibbs**

1 - inicialize o contador de iterações da cadeia, ou seja, tempo $t=0$; 

2 - Especifique valores iniciais $\theta^{(0)}=(\theta_1^{(0)}, \ldots, \theta_d^{(0)})$;

3 - Obtenha um novo valor de $\theta^{(t)}$ a partir de $\theta^{(t-1)}$ através da geração sucessiva dos valores 

$$\theta_1^{(t)} \sim h(\theta_1| \theta_2^{(t-1)}, \theta_3^{(t-1)}, \ldots, \theta_d^{(t-1)})$$
$$\theta_2^{(t)} \sim h(\theta_2| \theta_1^{(t)}, \theta_3^{(t-1)}, \ldots, \theta_d^{(t-1)})$$
$$\vdots$$
$$\theta_d^{(t)} \sim h(\theta_d| \theta_1^{(t)}, \theta_2^{(t)}, \ldots, \theta_{d-1}^{(t)})$$

4 - Incremente o contador de $t$ para $t+1$ e retorne ao passo 2 até obter convergência.   
    
    
**Obs.:** Note que o amostrador de Gibbs é um caso especial do algoritmo de Metropolis-Hastings, no qual os elementos de $\theta$ são atualizados um de cada vez (ou em bloco), tomando a distribuição condicional completa como proposta e probabilidade de aceitação igual a 1. 


## Exemplo 1     

Suponha que $X_1,X_2,...,X_n \sim N(\mu,\sigma^2)$ com $\mu$ e $\sigma^2$ desconhecidos. Definindo $\tau = \sigma^{-2}$, a função de verossimilhança é dada por

$$
L(\mu,\tau|\underline{x}) = \prod_{i=1}^n \frac{1}{\sqrt{2\pi}}\tau^{\frac{1}{2}}\exp\{-\frac{\tau}{2}(x_i - \mu)^2\} \\
\propto \tau^{\frac{n}{2}}\exp\{-\frac{\tau}{2} \sum (x_i - \mu)^2\}
$$
Especificando prioris independentes $\mu \sim N(0,\sigma_0^2)$, e $\tau \sim Gama(a,b)$, sendo $\sigma_0^2$, $a$ e $b$ conhecidos, segue que
$$
h(\mu,\tau) \propto h(\mu)h(\tau) \\
\propto \exp\Big\{-\frac{1}{2 \sigma_0^2}\mu^2\Big\}\tau^{a-1}e^{-b\tau} \\
$$
Logo, temos que
$$
\begin{aligned}
h(\mu,\tau|\underline{x}) \propto & h(\underline{x}|\mu,\tau)h(\mu,\tau) \\
\propto & \tau^{\frac{n}{2}}\exp\Big\{-\frac{\tau}{2}\sum (x_i-\mu)^2\Big\}\exp\Big\{-\frac{1}{2\sigma^2} \mu^2\Big\}\tau^{a-1}e^{-b\tau}
\end{aligned}
$$
esta distribuição conjunta não tem forma conhecida, mas após alguma manipulação algébrica, temos que as condicionais são

$$
h(\mu|\tau,\underline{x})\propto \exp\Big\{-\frac{1}{2c}(\mu - m)^2\Big\}
$$
sendo que
$$
  \begin{cases}
    c^{-1}       & \quad = n\tau+\frac{1}{\sigma_0^2}\\
    m  & \quad = n\tau \bar{x}c
  \end{cases}
$$
Portanto, $$\mu|\tau,\underline{x} \sim N(m,c)$$

A outra distribuição marginal é  

$$
h(\tau|\mu,\underline{x}) \propto \tau^{\frac{n}{2}+a-1}\exp\Big\{-(\frac{1}{2}\sum (x_i - \mu)^2 + b)\tau\Big\}
$$
Portanto, 

$$
\tau|\mu,\underline{x} \sim Gama\Big(\frac{n}{2}+a, \frac{1}{2}\sum (x_i - \mu)^2 + b\Big)
$$
Disto, o algoritmo do amostrador de Gibbs fica

1) Inicializa $\theta$ no tempo $t=0$, sendo que $\theta[t,1]$ representa o valor inicial para $\mu$, enquanto que $\theta[t,2]$ representa o valor inicial para $\tau$,

2) Para cada iteração indexada por $t=1,2,...,$ repetir,

  a) Gerar $\mu_t$ da distribuição $N(m,c)$ com $m= n\tau_{t-1}\bar{x}c$, sendo que 
  $c=(n\tau_{t-1} + \frac{1}{\sigma_0^2})^{-1}$, utilizando $\tau_{t-1}$ = $\theta[t-1,2]$,
  
  b) Gerar $\tau_t$ da distribuição $Gama(\alpha,\beta)$, com $\alpha = \frac{n}{2}+a$ e $\beta = \frac{1}{2}\sum (x_i - \mu_t)^2 + b$, usando para $\mu_t$ o valor gerado o passo anterior,
  
  d) Definir $\theta_t = (\mu_t,\tau_t)$,
  
  e) incrementar $t$.
  
**Exercício:** Gerar uma amostra de tamanho 100 da distribuição Normal(9,4). Adotando as contas dadas anteriormente com $\mu \sim Normal(0, \sigma_0 = 10)$ e $\tau \sim Gama(a = 8, b = 1)$, gerar uma amostra aleatória para $\mu,\tau|\underline{x}$ de tamanho 2000, calcule a média a posteriori.

```{r}
set.seed(2022)
n = 100
x = round(rnorm(n,9,2),2)
# medidas resumo 
summary(x)
var(x)


a = 8
b = 1
sigma0 = 10
tamanho = 4000
X<- matrix(NA,nrow = tamanho, ncol = 2)
X[1,] = c(0,1) # chute inicial mu=0 e tau=1
for (t in 2:tamanho) {
  tau <- X[t-1,2]
  c <- (n*tau + (1/(sigma0^2)))^(-1)
  m <- n*tau*mean(x)*c
  mu <- rnorm(1,m,sqrt(c))
  
  alpha <- n/2 + a
  beta <- b+ 1/2 * sum((x - mu)^2)
  tau <- rgamma(1,shape = alpha,scale = 1/beta)
  
  X[t,] <- c(mu,tau)
}

## plot das cadeias 
par(mfrow = c(1,2))
plot(X[,1], type = "l")
plot(X[,2], type = "l")

## acf das cadeias 
par(mfrow = c(1,2))
acf(X[,1])
acf(X[,2])

## Burn-in e saltos 
X_final = X[seq(1001, tamanho, 3),]

# plot da cadeia com burn-in e salto 
par(mfrow = c(1,2))
plot(X_final[,1], type = "l")
plot(X_final[,2], type = "l")

# acf das cadeias com burn-in e salto 
par(mfrow = c(1,2))
acf(X_final[,1])
acf(X_final[,2])

# plot das densidades 
par(mfrow = c(1,2))
hist(X_final[,1],probability = T)
hist(X_final[,2],probability = T)

# para mu
summary(X_final[,1])

# para tau
summary(X_final[,2])

# para sigma2

summary(1/X_final[,2])

```


